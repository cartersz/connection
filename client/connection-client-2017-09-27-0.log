[2017-09-27 12:04:36.959] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 18] - cloud connection service starting...
[2017-09-27 12:04:37.018] [INFO ] [main] [org.springframework.context.support.ClassPathXmlApplicationContext : 583] - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@68be2bc2: startup date [Wed Sep 27 12:04:37 CST 2017]; root of context hierarchy
[2017-09-27 12:04:37.098] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-server.xml]
[2017-09-27 12:04:37.277] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-consumer.xml]
[2017-09-27 12:04:37.316] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-producer.xml]
[2017-09-27 12:04:37.595] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 44] - Starting TCP Server...
[2017-09-27 12:04:37.692] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 60] - start TCP server 192.168.6.89 successfully on port 10010
[2017-09-27 12:04:37.762] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.838] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.886] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:04:37.886] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:04:37.890] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.891] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.895] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:04:37.895] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:04:37.896] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.897] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:04:37.901] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:04:37.901] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:04:37.905] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 12:04:37.908] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 12:04:37.909] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 21] - cloud connection service started
[2017-09-27 12:04:37.995] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:04:37.995] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:04:37.998] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:04:38.009] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:04:38.013] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 41
[2017-09-27 12:04:38.013] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 41
[2017-09-27 12:04:38.014] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 12:04:38.014] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 12:04:38.022] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0, connection-test-1]
[2017-09-27 12:04:38.022] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 12:07:27.274] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 18] - cloud connection service starting...
[2017-09-27 12:07:27.311] [INFO ] [main] [org.springframework.context.support.ClassPathXmlApplicationContext : 583] - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@d2cc05a: startup date [Wed Sep 27 12:07:27 CST 2017]; root of context hierarchy
[2017-09-27 12:07:27.350] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-server.xml]
[2017-09-27 12:07:27.469] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-consumer.xml]
[2017-09-27 12:07:27.509] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-producer.xml]
[2017-09-27 12:07:27.742] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 44] - Starting TCP Server...
[2017-09-27 12:07:27.816] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 60] - start TCP server 192.168.6.90 successfully on port 10010
[2017-09-27 12:07:27.873] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.936] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.978] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:07:27.979] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:07:27.983] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.984] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.987] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:07:27.988] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:07:27.989] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.989] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 12:07:27.992] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 12:07:27.992] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 12:07:27.996] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 12:07:28.000] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 12:07:28.000] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 21] - cloud connection service started
[2017-09-27 12:07:28.085] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:07:28.085] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:07:28.087] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 12:07:28.087] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 12:07:28.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 12:07:28.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 12:07:28.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:07:28.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:07:32.530] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 42
[2017-09-27 12:07:32.530] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 42
[2017-09-27 12:07:32.532] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 12:07:32.532] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 12:07:32.542] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 12:07:32.542] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0, connection-test-1]
[2017-09-27 12:12:33.108] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:12:33.109] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 12:12:33.110] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 12:12:33.110] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:12:33.389] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 12:12:33.389] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 12:12:33.501] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-0, connection-test-1]
[2017-09-27 12:12:33.501] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-2]
[2017-09-27 12:12:33.502] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:12:33.502] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:12:33.507] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 43
[2017-09-27 12:12:33.507] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 43
[2017-09-27 12:12:33.508] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 43
[2017-09-27 12:12:33.508] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-1] for group 0
[2017-09-27 12:12:33.508] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 12:12:33.508] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0] for group 0
[2017-09-27 12:12:33.512] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0]
[2017-09-27 12:12:33.512] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 12:12:33.512] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-1]
[2017-09-27 12:37:23.003] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:23.005] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:23.005] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:38.989] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:38.990] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:38.995] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.002] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.002] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.002] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.011] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.011] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.012] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.012] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.013] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.013] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 12:37:42.123] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.123] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:42.125] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 12:37:45.129] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0] for group 0
[2017-09-27 12:37:45.129] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-1] for group 0
[2017-09-27 12:37:45.129] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 12:37:45.139] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-0]
[2017-09-27 12:37:45.139] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-1]
[2017-09-27 12:37:45.139] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-2]
[2017-09-27 12:37:45.139] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:37:45.140] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:37:45.140] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:37:45.144] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 12:37:45.148] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 46
[2017-09-27 12:37:45.148] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 46
[2017-09-27 12:37:45.148] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 46
[2017-09-27 12:37:45.149] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0] for group 0
[2017-09-27 12:37:45.149] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 12:37:45.149] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-1] for group 0
[2017-09-27 12:37:45.152] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0]
[2017-09-27 12:37:45.152] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-1]
[2017-09-27 12:37:45.152] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 13:35:00.006] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:00.007] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:00.007] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:16.079] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:16.079] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:16.079] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.086] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.086] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.086] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.092] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.092] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.092] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.092] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.093] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.093] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 13:35:19.199] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.199] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:19.199] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 13:35:22.205] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 13:35:22.205] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0] for group 0
[2017-09-27 13:35:22.205] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-1] for group 0
[2017-09-27 13:35:23.073] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-1]
[2017-09-27 13:35:23.073] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-0]
[2017-09-27 13:35:23.073] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-2]
[2017-09-27 13:35:23.073] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 13:35:23.074] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 13:35:23.074] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 13:35:23.078] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 13:35:23.083] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 49
[2017-09-27 13:35:23.083] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 49
[2017-09-27 13:35:23.083] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 49
[2017-09-27 13:35:23.083] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0] for group 0
[2017-09-27 13:35:23.084] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 13:35:23.084] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-1] for group 0
[2017-09-27 13:35:23.087] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-1]
[2017-09-27 13:35:23.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0]
[2017-09-27 13:35:23.088] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 16:42:12.910] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:12.971] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:13.021] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:42:13.021] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:42:38.675] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:38.738] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:38.783] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:42:38.783] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:42:49.493] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:49.553] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:42:49.598] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:42:49.598] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:55:07.649] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:55:07.735] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:55:07.803] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:55:07.803] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:55:40.324] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:55:40.384] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:55:40.431] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:55:40.432] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:56:04.668] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 18] - cloud connection service starting...
[2017-09-27 16:56:04.704] [INFO ] [main] [org.springframework.context.support.ClassPathXmlApplicationContext : 583] - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@d2cc05a: startup date [Wed Sep 27 16:56:04 CST 2017]; root of context hierarchy
[2017-09-27 16:56:04.741] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-server.xml]
[2017-09-27 16:56:04.829] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-consumer.xml]
[2017-09-27 16:56:04.862] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-producer.xml]
[2017-09-27 16:56:05.110] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 44] - Starting TCP Server...
[2017-09-27 16:56:05.226] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 60] - start TCP server 192.168.6.89 successfully on port 10010
[2017-09-27 16:56:05.309] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.395] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.436] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:56:05.437] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:56:05.444] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.445] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.450] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:56:05.450] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:56:05.451] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.452] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:05.455] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:56:05.456] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:56:05.459] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 16:56:05.463] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 16:56:05.464] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 21] - cloud connection service started
[2017-09-27 16:56:05.560] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 16:56:05.560] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 16:56:05.562] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 16:56:05.562] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 16:56:05.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 16:56:05.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 16:56:05.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 16:56:05.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 16:56:05.573] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 16:56:05.578] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 52
[2017-09-27 16:56:05.578] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 52
[2017-09-27 16:56:05.579] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 16:56:05.579] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 16:56:05.586] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 16:56:05.586] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0, connection-test-1]
[2017-09-27 16:56:42.138] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:42.204] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:56:42.253] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:56:42.253] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 16:59:10.878] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:59:10.937] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 16:59:10.981] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 16:59:10.981] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:01:10.580] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:01:10.581] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 17:01:10.581] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 17:01:10.581] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:01:12.396] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 17:01:12.483] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-0, connection-test-1]
[2017-09-27 17:01:12.484] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:01:12.514] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 17:01:13.485] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[connection-test-2]
[2017-09-27 17:01:13.485] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:01:13.490] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 53
[2017-09-27 17:01:13.490] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 53
[2017-09-27 17:01:13.490] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 53
[2017-09-27 17:01:13.491] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:01:13.491] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0] for group 0
[2017-09-27 17:01:13.491] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-1] for group 0
[2017-09-27 17:01:13.495] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-1-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-1]
[2017-09-27 17:01:13.496] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0]
[2017-09-27 17:01:13.496] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 17:02:22.732] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 18] - cloud connection service starting...
[2017-09-27 17:02:22.770] [INFO ] [main] [org.springframework.context.support.ClassPathXmlApplicationContext : 583] - Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@d2cc05a: startup date [Wed Sep 27 17:02:22 CST 2017]; root of context hierarchy
[2017-09-27 17:02:22.808] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-server.xml]
[2017-09-27 17:02:22.916] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-consumer.xml]
[2017-09-27 17:02:22.950] [INFO ] [main] [org.springframework.beans.factory.xml.XmlBeanDefinitionReader : 317] - Loading XML bean definitions from class path resource [spring-kafka-producer.xml]
[2017-09-27 17:02:23.207] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 44] - Starting TCP Server...
[2017-09-27 17:02:23.287] [INFO ] [main] [com.orvibo.cloud.connection.server.tcp.netty.NettyTCPServer : 60] - start TCP server 192.168.6.89 successfully on port 10010
[2017-09-27 17:02:23.339] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.400] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.448] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:02:23.448] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:02:23.453] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.455] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-2
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.459] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:02:23.460] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:02:23.461] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.462] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-3
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:23.466] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:02:23.467] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:02:23.470] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 17:02:23.474] [INFO ] [main] [org.springframework.context.support.DefaultLifecycleProcessor : 343] - Starting beans in phase 0
[2017-09-27 17:02:23.475] [INFO ] [main] [com.orvibo.cloud.connection.server.Main : 21] - cloud connection service started
[2017-09-27 17:02:23.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:02:23.563] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:02:23.565] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 17:02:23.565] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 17:02:23.566] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 17:02:23.566] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 242] - partitions revoked:[]
[2017-09-27 17:02:23.566] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:02:23.566] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:02:25.555] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 54
[2017-09-27 17:02:25.555] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 54
[2017-09-27 17:02:25.556] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:02:25.556] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-1] for group 0
[2017-09-27 17:02:25.567] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-2-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-2]
[2017-09-27 17:02:25.567] [INFO ] [connectionTestTopicConcurrentMessageListenerContainer-0-C-1] [org.springframework.kafka.listener.ConcurrentMessageListenerContainer : 247] - partitions assigned:[connection-test-0, connection-test-1]
[2017-09-27 17:02:27.578] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:27.646] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:02:27.693] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:02:27.693] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:07:23.907] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:07:23.966] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:07:24.014] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:07:24.014] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:08:58.764] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:08:58.823] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:08:58.867] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:08:58.868] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:09:32.315] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:09:32.375] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:09:32.423] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:09:32.423] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:09:44.901] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:09:45.007] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:09:45.076] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:09:45.076] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:10:06.239] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:10:06.324] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:10:06.393] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:10:06.394] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:11:18.118] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:11:18.199] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:11:18.257] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:11:18.257] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:12:42.615] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:12:42.693] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:12:42.757] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:12:42.758] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:13:13.716] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:13:13.776] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:13:13.824] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:13:13.824] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:15:14.890] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:15:14.950] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:15:14.997] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:15:14.997] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:15:15.122] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:15:15.126] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 17:15:15.126] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:15:17.898] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 63
[2017-09-27 17:15:17.900] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:17:32.787] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:17:32.861] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:17:32.921] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:17:32.922] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:18:05.610] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:18:05.692] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 17:18:05.748] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 17:18:05.748] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 17:18:26.893] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:18:26.904] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 17:18:26.905] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:18:27.326] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 65
[2017-09-27 17:18:27.331] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:19:47.441] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 17:19:51.752] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:19:51.780] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 622] - Auto-commit of offsets {connection-test-2=OffsetAndMetadata{offset=9, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 17:19:51.782] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 648] - Auto-commit of offsets {connection-test-2=OffsetAndMetadata{offset=9, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 17:19:51.783] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 17:19:51.784] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:19:54.522] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 67
[2017-09-27 17:19:54.524] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:27:17.671] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 17:27:17.675] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 17:27:17.679] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 622] - Auto-commit of offsets {connection-test-2=OffsetAndMetadata{offset=9, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 17:27:17.681] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 648] - Auto-commit of offsets {connection-test-2=OffsetAndMetadata{offset=9, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 17:27:17.682] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 17:27:17.684] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 17:27:19.509] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 69
[2017-09-27 17:27:19.511] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 17:53:51.216] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 18:11:18.657] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:11:18.720] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:11:18.779] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:11:18.779] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:11:18.943] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:11:18.947] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:11:18.947] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:11:21.589] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 71
[2017-09-27 18:11:21.591] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-2] for group 0
[2017-09-27 18:12:13.762] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-2] for group 0
[2017-09-27 18:12:13.762] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:12:13.766] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 72
[2017-09-27 18:12:13.767] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:14:31.388] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:14:31.449] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:14:31.497] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:14:31.498] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:14:56.163] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:14:56.260] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:14:56.355] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:14:56.356] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:19:32.653] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:19:32.856] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:19:33.043] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:19:33.044] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:19:49.457] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:19:49.469] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:19:49.469] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:22:48.225] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:22:48.300] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:22:48.382] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:22:48.382] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:24:41.140] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:24:41.202] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:24:41.260] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:24:41.261] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:24:41.404] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:24:41.407] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:24:41.408] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:24:41.419] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 78
[2017-09-27 18:24:41.420] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:27:27.020] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:27:27.134] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:27:27.204] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:27:27.206] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:28:01.360] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:28:01.448] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:28:01.526] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:28:01.526] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:28:07.687] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:28:07.697] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:28:07.698] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:28:07.726] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 80
[2017-09-27 18:28:07.729] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:30:50.445] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 18:30:50.449] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:30:50.472] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 622] - Auto-commit of offsets {connection-test-0=OffsetAndMetadata{offset=7, metadata=''}, connection-test-2=OffsetAndMetadata{offset=10, metadata=''}, connection-test-1=OffsetAndMetadata{offset=10, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 18:30:50.474] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 648] - Auto-commit of offsets {connection-test-0=OffsetAndMetadata{offset=7, metadata=''}, connection-test-2=OffsetAndMetadata{offset=10, metadata=''}, connection-test-1=OffsetAndMetadata{offset=10, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 18:30:50.475] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:30:50.477] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:30:50.485] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 82
[2017-09-27 18:30:50.486] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:31:43.324] [INFO ] [kafka-coordinator-heartbeat-thread | 0] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 618] - Marking the coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) dead for group 0
[2017-09-27 18:31:43.327] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:31:43.329] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 622] - Auto-commit of offsets {connection-test-0=OffsetAndMetadata{offset=7, metadata=''}, connection-test-2=OffsetAndMetadata{offset=10, metadata=''}, connection-test-1=OffsetAndMetadata{offset=10, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 18:31:43.330] [WARN ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 648] - Auto-commit of offsets {connection-test-0=OffsetAndMetadata{offset=7, metadata=''}, connection-test-2=OffsetAndMetadata{offset=10, metadata=''}, connection-test-1=OffsetAndMetadata{offset=10, metadata=''}} failed for group 0: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
[2017-09-27 18:31:43.330] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:31:43.331] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:31:43.335] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 84
[2017-09-27 18:31:43.336] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:31:49.290] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:31:49.376] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:31:49.431] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:31:49.431] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:31:54.908] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:31:54.918] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:31:54.919] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:32:13.357] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 85
[2017-09-27 18:32:13.361] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:33:18.059] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:33:18.124] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:33:18.177] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:33:18.177] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:33:18.328] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:33:18.331] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:33:18.331] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:33:18.371] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 87
[2017-09-27 18:33:18.373] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
[2017-09-27 18:33:37.860] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:33:37.925] [INFO ] [main] [org.apache.kafka.clients.consumer.ConsumerConfig : 180] - ConsumerConfig values: 
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [192.168.2.201:9092, 192.168.2.202:9092, 192.168.2.192:9092]
	check.crcs = true
	client.id = consumer-1
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[2017-09-27 18:33:37.978] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 83] - Kafka version : 0.10.2.0
[2017-09-27 18:33:37.978] [INFO ] [main] [org.apache.kafka.common.utils.AppInfoParser : 84] - Kafka commitId : 576d93a8dc0cf421
[2017-09-27 18:33:38.112] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 573] - Discovered coordinator 192.168.2.202:9092 (id: 2147483646 rack: null) for group 0.
[2017-09-27 18:33:38.115] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 393] - Revoking previously assigned partitions [] for group 0
[2017-09-27 18:33:38.116] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 407] - (Re-)joining group 0
[2017-09-27 18:33:54.505] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.AbstractCoordinator : 375] - Successfully joined group 0 with generation 88
[2017-09-27 18:33:54.507] [INFO ] [main] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator : 252] - Setting newly assigned partitions [connection-test-0, connection-test-2, connection-test-1] for group 0
